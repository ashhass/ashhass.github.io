<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-08T21:10:08+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ayda Sultan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Circuits</title><link href="http://localhost:4000/2025/05/08/Circuits.html" rel="alternate" type="text/html" title="Circuits" /><published>2025-05-08T00:00:00+03:00</published><updated>2025-05-08T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/08/Circuits</id><content type="html" xml:base="http://localhost:4000/2025/05/08/Circuits.html"><![CDATA[<p>Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?</p>

<p>Circuits represent local subgraphs. Whether they actually exist in networks is a matter of scrutiny, but significant evidence show that there are many model-agnostic circuits from which we can extract understandable algorithms (the weights actually do mean something!). Utilizing techniques such as activation patching (to be discussed in a future post), ablation and so forth we can test for the feasibility of structure within neural networks.</p>

<p>There are voices arguing that circuits are human imposed and that networks use more of distributed representations where every neuron contributes a little to many behaviors without any meaningful structure. Maybe or maybe not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?]]></summary></entry><entry><title type="html">Why mechanistic?</title><link href="http://localhost:4000/2025/05/07/Why-mechanistic.html" rel="alternate" type="text/html" title="Why mechanistic?" /><published>2025-05-07T00:00:00+03:00</published><updated>2025-05-07T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/07/Why%20mechanistic</id><content type="html" xml:base="http://localhost:4000/2025/05/07/Why-mechanistic.html"><![CDATA[<p>There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.</p>

<p>Neural networks are massive, entangled and hard to decode. Playing cause and effect with only model outputs and inputs seem like a much more feasible and productive pursuit. The problem is that the resulting explanations are often unfaithful, incomplete and do not allow for correcting learned behavior. An even bigger problem is that the explanations themselves need to be explained. If a saliency method attributes certain behavior to some particular patch of an input image, what features exactly within that patch trigger the attribution?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.]]></summary></entry><entry><title type="html">Superposition</title><link href="http://localhost:4000/2025/05/06/Superposition.html" rel="alternate" type="text/html" title="Superposition" /><published>2025-05-06T00:00:00+03:00</published><updated>2025-05-06T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/06/Superposition</id><content type="html" xml:base="http://localhost:4000/2025/05/06/Superposition.html"><![CDATA[<p>Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.]]></summary></entry><entry><title type="html">What on mars is a feature?</title><link href="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html" rel="alternate" type="text/html" title="What on mars is a feature?" /><published>2025-05-05T00:00:00+03:00</published><updated>2025-05-05T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/05/What%20on%20Mars%20is%20a%20Feature</id><content type="html" xml:base="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html"><![CDATA[<p>Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.</p>

<p>The idea of features was created as a quest to define atomic units of analysis for neural network computations. This means that features are not decomposable to other meaningful units. In the activation space, features are represented usually as interpretable directions but can also be abstract. Features can be represented by multiple neurons working together. In some cases, individual neurons can also encode features.</p>

<p>So, do features equate to concepts? No, features are not about you, they’re about the model. Whatever the model uses that causally affects performance is considered a feature. Sometimes it does encode some human-interpretable concept, but many times it does not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.]]></summary></entry><entry><title type="html">What is mechanistic interpretability?</title><link href="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html" rel="alternate" type="text/html" title="What is mechanistic interpretability?" /><published>2025-05-04T00:00:00+03:00</published><updated>2025-05-04T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/04/What%20is%20Mechanistic%20Interpretability</id><content type="html" xml:base="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html"><![CDATA[<p>If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).</p>

<p>This is very much an oversimplification and gives an idealized view of neural networks. Some of the reasons why this is difficult—borderline impossible—is that a lot of the variables networks learn are entangled. One neuron might correspond to multiple concepts. Or there might be learned features we don’t understand or perhaps there are hidden features that the model uses but we can’t extract. On a philosophical point, that’s a bit similar to why we can’t accurately predict the future. We might think that a specific event is certain because we have controlled all the variables but the future might be dependent on causal variables we don’t know exist (latent variables).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).]]></summary></entry></feed>