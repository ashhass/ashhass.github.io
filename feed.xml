<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-17T22:42:23+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ayda Sultan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Mi on token embeddings</title><link href="http://localhost:4000/2025/05/17/MI-on-Token-Embeddings.html" rel="alternate" type="text/html" title="Mi on token embeddings" /><published>2025-05-17T00:00:00+03:00</published><updated>2025-05-17T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/17/MI%20on%20Token%20Embeddings</id><content type="html" xml:base="http://localhost:4000/2025/05/17/MI-on-Token-Embeddings.html"><![CDATA[<p>Features are thought to be directions in the embedding space. The superposition problem states that a single direction might encode multiple features. One direction can encode seemingly unrelated concepts like savannah and carpets. That might not be the case for all directions. In order to figure out what each direction corresponds to, we work with the token embedding matrices. One plausible way is to sparsify the embedding matrix by either introducing a sparsity constraint during training or choosing the top-k largest magnitudes. The way to enforce a sparsiy constraint is by adding a term to the loss function that penalizes any non-zero entry. Gradient descent therefore drives many coordinates toward exactly 0 so long as the reconstruction error stays acceptable.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Features are thought to be directions in the embedding space. The superposition problem states that a single direction might encode multiple features. One direction can encode seemingly unrelated concepts like savannah and carpets. That might not be the case for all directions. In order to figure out what each direction corresponds to, we work with the token embedding matrices. One plausible way is to sparsify the embedding matrix by either introducing a sparsity constraint during training or choosing the top-k largest magnitudes. The way to enforce a sparsiy constraint is by adding a term to the loss function that penalizes any non-zero entry. Gradient descent therefore drives many coordinates toward exactly 0 so long as the reconstruction error stays acceptable.]]></summary></entry><entry><title type="html">Mech interp on token embeddings</title><link href="http://localhost:4000/2025/05/17/Mech-Interp-on-Token-Embeddings.html" rel="alternate" type="text/html" title="Mech interp on token embeddings" /><published>2025-05-17T00:00:00+03:00</published><updated>2025-05-17T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/17/Mech%20Interp%20on%20Token%20Embeddings</id><content type="html" xml:base="http://localhost:4000/2025/05/17/Mech-Interp-on-Token-Embeddings.html"><![CDATA[<p>Features are thought to be directions in the embedding space. The superposition problem states that a single direction might encode multiple features. One direction can encode seemingly unrelated concepts like savannah and carpets. That might not be the case for all directions. In order to figure out what each direction corresponds to, we work with the token embedding matrices. One plausible way is to sparsify the embedding matrix by either introducing a sparsity constraint during training or choosing the top-k largest magnitudes. The way to enforce a sparsiy constraint is by adding a term to the loss function that penalizes any non-zero entry. Gradient descent therefore drives many coordinates toward exactly 0 so long as the reconstruction error stays acceptable.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Features are thought to be directions in the embedding space. The superposition problem states that a single direction might encode multiple features. One direction can encode seemingly unrelated concepts like savannah and carpets. That might not be the case for all directions. In order to figure out what each direction corresponds to, we work with the token embedding matrices. One plausible way is to sparsify the embedding matrix by either introducing a sparsity constraint during training or choosing the top-k largest magnitudes. The way to enforce a sparsiy constraint is by adding a term to the loss function that penalizes any non-zero entry. Gradient descent therefore drives many coordinates toward exactly 0 so long as the reconstruction error stays acceptable.]]></summary></entry><entry><title type="html">Activation functions</title><link href="http://localhost:4000/2025/05/14/Activation-Functions.html" rel="alternate" type="text/html" title="Activation functions" /><published>2025-05-14T00:00:00+03:00</published><updated>2025-05-14T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/14/Activation%20Functions</id><content type="html" xml:base="http://localhost:4000/2025/05/14/Activation-Functions.html"><![CDATA[<p>How do activation functions affect the degree of model interpretability? Activation functions provide models with higher capability to represent non-linear functions. But, introducing non-linearity makes it harder to trace the flow of information through the network. One upside of activation functions, like ReLU, is that it increases the sparsity of outputs making it easier to discard irrelevant neurons and attribute features more confidently to activated neurons. This is because ReLU diminishes weakly activated neurons to 0. Functions like sigmoid produce saturated values which result in distributed representations where many neurons weakly contribute to representing certain features. This makes it seem that every neuron is contributing slightly to every concept ultimately resulting in superposition.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How do activation functions affect the degree of model interpretability? Activation functions provide models with higher capability to represent non-linear functions. But, introducing non-linearity makes it harder to trace the flow of information through the network. One upside of activation functions, like ReLU, is that it increases the sparsity of outputs making it easier to discard irrelevant neurons and attribute features more confidently to activated neurons. This is because ReLU diminishes weakly activated neurons to 0. Functions like sigmoid produce saturated values which result in distributed representations where many neurons weakly contribute to representing certain features. This makes it seem that every neuron is contributing slightly to every concept ultimately resulting in superposition.]]></summary></entry><entry><title type="html">Example setup</title><link href="http://localhost:4000/2025/05/13/Example-Setup.html" rel="alternate" type="text/html" title="Example setup" /><published>2025-05-13T00:00:00+03:00</published><updated>2025-05-13T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/13/Example%20Setup</id><content type="html" xml:base="http://localhost:4000/2025/05/13/Example-Setup.html"><![CDATA[<p>To put concepts into a practical example, we can try to simulate a trivial example to understand the general flow. We can extract the Sobel horizontal filter from a CNN trained to mimic an edge detection algorithm.</p>

<p>To clarify the components, the input would be a 28 by 28 image, the kernel would be a 3 by 3 filter akin to the Sobel filter, padding would be 1. We generate the ground truth by applying an edge detection algorithm to the images.</p>

<p>The point is to observe whether the weights converge to Sobel-like filters. Weights in this case refer to the connections between the 3 by 3 image patch to the hidden layer which consists of a single neuron. This neuron is the output of applying the weights on the image patch (also called the activation). We can visualize this activation to see whether an edge is detected in the patch.</p>

<p>Now, what happens if there are more neurons than necessary to extract the algorithm? What happens if there are fewer neurons? What happens when we ablate some filter values? Can we patch in these Sobel-like filters into any trained CNN architecture?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[To put concepts into a practical example, we can try to simulate a trivial example to understand the general flow. We can extract the Sobel horizontal filter from a CNN trained to mimic an edge detection algorithm.]]></summary></entry><entry><title type="html">From neurons to circuits</title><link href="http://localhost:4000/2025/05/11/From-Neurons-to-Circuits.html" rel="alternate" type="text/html" title="From neurons to circuits" /><published>2025-05-11T00:00:00+03:00</published><updated>2025-05-11T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/11/From%20Neurons%20to%20Circuits</id><content type="html" xml:base="http://localhost:4000/2025/05/11/From-Neurons-to-Circuits.html"><![CDATA[<p>Each neuron in a neural network contains some sort of information. The weights corresponding to each neuron dictate the contribution strength of a specific neuron to the formation of more complicated neurons in subsequent layers. Now, certain groups of neurons work together to form concepts. These groups of neurons and the connections between them form circuits. But how are neurons grouped into circuits? Grouping neurons should be based on concepts/features that they fire for. After choosing some behavior, weights can be traced back to components that influence this activation.</p>

<p>To isolate causal neurons, activations from current prompt are replaced with activations from a control prompt that does not trigger the behavior in question. It’s important to make sure that no other component is causing this behavior change by controlling all other changes. This process is called activation patching. We can also ablate components or zero out weights of certain neurons and observe whether any behavior change is present. This process is called ablation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Each neuron in a neural network contains some sort of information. The weights corresponding to each neuron dictate the contribution strength of a specific neuron to the formation of more complicated neurons in subsequent layers. Now, certain groups of neurons work together to form concepts. These groups of neurons and the connections between them form circuits. But how are neurons grouped into circuits? Grouping neurons should be based on concepts/features that they fire for. After choosing some behavior, weights can be traced back to components that influence this activation.]]></summary></entry><entry><title type="html">The rise of mechanistic interpretability</title><link href="http://localhost:4000/2025/05/09/The-Rise-of-Mechanistic-Interpretability.html" rel="alternate" type="text/html" title="The rise of mechanistic interpretability" /><published>2025-05-09T00:00:00+03:00</published><updated>2025-05-09T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/09/The%20Rise%20of%20Mechanistic%20Interpretability</id><content type="html" xml:base="http://localhost:4000/2025/05/09/The-Rise-of-Mechanistic-Interpretability.html"><![CDATA[<p>I recently read a twitter thread about the growing interest of pre-PhD students on mechanistic interpretability. A lot of non-pre-PhD researchers speculated this could be due to how the field is marketed and the fact that there’s a lower barrier to entry in terms of compute resources.  What no one mentioned is how baffling the idea of building and deploying systems that nobody understands into the wild is to a newcomer to the field. Building abstract systems that ‘work’ (on a tiny, carefully curated distribution) and only later attempting to decode their inner workings is one thing, but arguing these systems would never be understood, as if by divine decree, is something else entirely. Without evidence, that’s a belief not a fact. Trying out different hyperparameter combinations, folding hands in prayer and making sure not to touch anything that works without understanding why it works does not sound like the ‘science’ pre-phd students imagine themselves doing, I think.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I recently read a twitter thread about the growing interest of pre-PhD students on mechanistic interpretability. A lot of non-pre-PhD researchers speculated this could be due to how the field is marketed and the fact that there’s a lower barrier to entry in terms of compute resources. What no one mentioned is how baffling the idea of building and deploying systems that nobody understands into the wild is to a newcomer to the field. Building abstract systems that ‘work’ (on a tiny, carefully curated distribution) and only later attempting to decode their inner workings is one thing, but arguing these systems would never be understood, as if by divine decree, is something else entirely. Without evidence, that’s a belief not a fact. Trying out different hyperparameter combinations, folding hands in prayer and making sure not to touch anything that works without understanding why it works does not sound like the ‘science’ pre-phd students imagine themselves doing, I think.]]></summary></entry><entry><title type="html">Circuits</title><link href="http://localhost:4000/2025/05/08/Circuits.html" rel="alternate" type="text/html" title="Circuits" /><published>2025-05-08T00:00:00+03:00</published><updated>2025-05-08T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/08/Circuits</id><content type="html" xml:base="http://localhost:4000/2025/05/08/Circuits.html"><![CDATA[<p>Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?</p>

<p>Circuits represent local subgraphs. Whether they actually exist in networks is a matter of scrutiny, but significant evidence show that there are many model-agnostic circuits from which we can extract understandable algorithms (the weights actually do mean something!). Utilizing techniques such as activation patching (to be discussed in a future post), ablation and so forth we can test for the feasibility of structure within neural networks.</p>

<p>There are voices arguing that circuits are human imposed and that networks use more of distributed representations where every neuron contributes a little to many behaviors without any meaningful structure. Maybe or maybe not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?]]></summary></entry><entry><title type="html">Why mechanistic?</title><link href="http://localhost:4000/2025/05/07/Why-mechanistic.html" rel="alternate" type="text/html" title="Why mechanistic?" /><published>2025-05-07T00:00:00+03:00</published><updated>2025-05-07T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/07/Why%20mechanistic</id><content type="html" xml:base="http://localhost:4000/2025/05/07/Why-mechanistic.html"><![CDATA[<p>There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.</p>

<p>Neural networks are massive, entangled and hard to decode. Playing cause and effect with only model outputs and inputs seem like a much more feasible and productive pursuit. The problem is that the resulting explanations are often unfaithful, incomplete and do not allow for correcting learned behavior. An even bigger problem is that the explanations themselves need to be explained. If a saliency method attributes certain behavior to some particular patch of an input image, what features exactly within that patch trigger the attribution?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.]]></summary></entry><entry><title type="html">Superposition</title><link href="http://localhost:4000/2025/05/06/Superposition.html" rel="alternate" type="text/html" title="Superposition" /><published>2025-05-06T00:00:00+03:00</published><updated>2025-05-06T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/06/Superposition</id><content type="html" xml:base="http://localhost:4000/2025/05/06/Superposition.html"><![CDATA[<p>Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.]]></summary></entry><entry><title type="html">What on mars is a feature?</title><link href="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html" rel="alternate" type="text/html" title="What on mars is a feature?" /><published>2025-05-05T00:00:00+03:00</published><updated>2025-05-05T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/05/What%20on%20Mars%20is%20a%20Feature</id><content type="html" xml:base="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html"><![CDATA[<p>Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.</p>

<p>The idea of features was created as a quest to define atomic units of analysis for neural network computations. This means that features are not decomposable to other meaningful units. In the activation space, features are represented usually as interpretable directions but can also be abstract. Features can be represented by multiple neurons working together. In some cases, individual neurons can also encode features.</p>

<p>So, do features equate to concepts? No, features are not about you, they’re about the model. Whatever the model uses that causally affects performance is considered a feature. Sometimes it does encode some human-interpretable concept, but many times it does not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.]]></summary></entry></feed>