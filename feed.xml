<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-13T23:55:30+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ayda Sultan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Example setup</title><link href="http://localhost:4000/2025/05/13/Example-Setup.html" rel="alternate" type="text/html" title="Example setup" /><published>2025-05-13T00:00:00+03:00</published><updated>2025-05-13T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/13/Example%20Setup</id><content type="html" xml:base="http://localhost:4000/2025/05/13/Example-Setup.html"><![CDATA[<p>To put concepts into a practical example, we can try to simulate a trivial example to understand the general flow. We can extract the Sobel horizontal filter from a CNN trained to mimic an edge detection algorithm.</p>

<p>To clarify the components, the input would be a 28 by 28 image, the kernel would be a 3 by 3 filter akin to the Sobel filter, padding would be 1. We generate the ground truth by applying an edge detection algorithm to the images.</p>

<p>The point is to observe whether the weights converge to Sobel-like filters. Weights in this case refer to the connections between the 3 by 3 image patch to the hidden layer which consists of a single neuron. This neuron is the output of applying the weights on the image patch (also called the activation). We can visualize this activation to see whether an edge is detected in the patch.</p>

<p>Now, what happens if there are more neurons than necessary to extract the algorithm? What happens if there are fewer neurons? What happens when we ablate some filter values? Can we patch in these Sobel-like filters into any trained CNN architecture?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[To put concepts into a practical example, we can try to simulate a trivial example to understand the general flow. We can extract the Sobel horizontal filter from a CNN trained to mimic an edge detection algorithm.]]></summary></entry><entry><title type="html">From neurons to circuits</title><link href="http://localhost:4000/2025/05/11/From-Neurons-to-Circuits.html" rel="alternate" type="text/html" title="From neurons to circuits" /><published>2025-05-11T00:00:00+03:00</published><updated>2025-05-11T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/11/From%20Neurons%20to%20Circuits</id><content type="html" xml:base="http://localhost:4000/2025/05/11/From-Neurons-to-Circuits.html"><![CDATA[<p>Each neuron in a neural network contains some sort of information. The weights corresponding to each neuron dictate the contribution strength of a specific neuron to the formation of more complicated neurons in subsequent layers. Now, certain groups of neurons work together to form concepts. These groups of neurons and the connections between them form circuits. But how are neurons grouped into circuits? Grouping neurons should be based on concepts/features that they fire for. After choosing some behavior, weights can be traced back to components that influence this activation.</p>

<p>To isolate causal neurons, activations from current prompt are replaced with activations from a control prompt that does not trigger the behavior in question. It’s important to make sure that no other component is causing this behavior change by controlling all other changes. This process is called activation patching. We can also ablate components or zero out weights of certain neurons and observe whether any behavior change is present. This process is called ablation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Each neuron in a neural network contains some sort of information. The weights corresponding to each neuron dictate the contribution strength of a specific neuron to the formation of more complicated neurons in subsequent layers. Now, certain groups of neurons work together to form concepts. These groups of neurons and the connections between them form circuits. But how are neurons grouped into circuits? Grouping neurons should be based on concepts/features that they fire for. After choosing some behavior, weights can be traced back to components that influence this activation.]]></summary></entry><entry><title type="html">The rise of mechanistic interpretability</title><link href="http://localhost:4000/2025/05/09/The-Rise-of-Mechanistic-Interpretability.html" rel="alternate" type="text/html" title="The rise of mechanistic interpretability" /><published>2025-05-09T00:00:00+03:00</published><updated>2025-05-09T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/09/The%20Rise%20of%20Mechanistic%20Interpretability</id><content type="html" xml:base="http://localhost:4000/2025/05/09/The-Rise-of-Mechanistic-Interpretability.html"><![CDATA[<p>I recently read a twitter thread about the growing interest of pre-PhD students on mechanistic interpretability. A lot of non-pre-PhD researchers speculated this could be due to how the field is marketed and the fact that there’s a lower barrier to entry in terms of compute resources.  What no one mentioned is how baffling the idea of building and deploying systems that nobody understands into the wild is to a newcomer to the field. Building abstract systems that ‘work’ (on a tiny, carefully curated distribution) and only later attempting to decode their inner workings is one thing, but arguing these systems would never be understood, as if by divine decree, is something else entirely. Without evidence, that’s a belief not a fact. Trying out different hyperparameter combinations, folding hands in prayer and making sure not to touch anything that works without understanding why it works does not sound like the ‘science’ pre-phd students imagine themselves doing, I think.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I recently read a twitter thread about the growing interest of pre-PhD students on mechanistic interpretability. A lot of non-pre-PhD researchers speculated this could be due to how the field is marketed and the fact that there’s a lower barrier to entry in terms of compute resources. What no one mentioned is how baffling the idea of building and deploying systems that nobody understands into the wild is to a newcomer to the field. Building abstract systems that ‘work’ (on a tiny, carefully curated distribution) and only later attempting to decode their inner workings is one thing, but arguing these systems would never be understood, as if by divine decree, is something else entirely. Without evidence, that’s a belief not a fact. Trying out different hyperparameter combinations, folding hands in prayer and making sure not to touch anything that works without understanding why it works does not sound like the ‘science’ pre-phd students imagine themselves doing, I think.]]></summary></entry><entry><title type="html">Circuits</title><link href="http://localhost:4000/2025/05/08/Circuits.html" rel="alternate" type="text/html" title="Circuits" /><published>2025-05-08T00:00:00+03:00</published><updated>2025-05-08T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/08/Circuits</id><content type="html" xml:base="http://localhost:4000/2025/05/08/Circuits.html"><![CDATA[<p>Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?</p>

<p>Circuits represent local subgraphs. Whether they actually exist in networks is a matter of scrutiny, but significant evidence show that there are many model-agnostic circuits from which we can extract understandable algorithms (the weights actually do mean something!). Utilizing techniques such as activation patching (to be discussed in a future post), ablation and so forth we can test for the feasibility of structure within neural networks.</p>

<p>There are voices arguing that circuits are human imposed and that networks use more of distributed representations where every neuron contributes a little to many behaviors without any meaningful structure. Maybe or maybe not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Features are the atomic, meaningful units in neural networks. Circuits are the connections between them. If we understand the meaning of each feature, the next question becomes can we understand how these features work together to construct higher-level concepts? Is there a meaningful structure through which information flows? Can we actually extract transferable mini-algorithms from this massive, entangled network?]]></summary></entry><entry><title type="html">Why mechanistic?</title><link href="http://localhost:4000/2025/05/07/Why-mechanistic.html" rel="alternate" type="text/html" title="Why mechanistic?" /><published>2025-05-07T00:00:00+03:00</published><updated>2025-05-07T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/07/Why%20mechanistic</id><content type="html" xml:base="http://localhost:4000/2025/05/07/Why-mechanistic.html"><![CDATA[<p>There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.</p>

<p>Neural networks are massive, entangled and hard to decode. Playing cause and effect with only model outputs and inputs seem like a much more feasible and productive pursuit. The problem is that the resulting explanations are often unfaithful, incomplete and do not allow for correcting learned behavior. An even bigger problem is that the explanations themselves need to be explained. If a saliency method attributes certain behavior to some particular patch of an input image, what features exactly within that patch trigger the attribution?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[There are lots of ways to interpret models. Previously, most of the focus was on interpreting behaviors of models by attributing behaviors to specific parts of the input. This was mostly the case in vision interpretability research. Interpretability for language models has been rebranded to ‘mechanistic’ interpretability but similar lines of work were already going on under a different name (and almost fully within academia). Read the paper ‘Mechanistic?’ by Saphra and Wiegreffe.]]></summary></entry><entry><title type="html">Superposition</title><link href="http://localhost:4000/2025/05/06/Superposition.html" rel="alternate" type="text/html" title="Superposition" /><published>2025-05-06T00:00:00+03:00</published><updated>2025-05-06T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/06/Superposition</id><content type="html" xml:base="http://localhost:4000/2025/05/06/Superposition.html"><![CDATA[<p>Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Like many machine learning subfields, mechanistic interpretability is filled with unnecessary jargon (assigning new names for things that already have names). Superposition is the idea that features can’t align with basis vectors because there are more features that need to be represented in a model than there are neurons. This leads to polysemanticity—one neuron firing for multiple input concepts. All of this is a result of a loss function designed to improve performance on tasks without interpretability regularizations. The model is free to construct its structure based on how much features it can compress within the given parameters. Since this phenomenon prevents us from interpreting model behavior by analyzing individual neurons, it becomes important to understand the geometrical structure of features in high-dimensional spaces.]]></summary></entry><entry><title type="html">What on mars is a feature?</title><link href="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html" rel="alternate" type="text/html" title="What on mars is a feature?" /><published>2025-05-05T00:00:00+03:00</published><updated>2025-05-05T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/05/What%20on%20Mars%20is%20a%20Feature</id><content type="html" xml:base="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html"><![CDATA[<p>Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.</p>

<p>The idea of features was created as a quest to define atomic units of analysis for neural network computations. This means that features are not decomposable to other meaningful units. In the activation space, features are represented usually as interpretable directions but can also be abstract. Features can be represented by multiple neurons working together. In some cases, individual neurons can also encode features.</p>

<p>So, do features equate to concepts? No, features are not about you, they’re about the model. Whatever the model uses that causally affects performance is considered a feature. Sometimes it does encode some human-interpretable concept, but many times it does not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.]]></summary></entry><entry><title type="html">What is mechanistic interpretability?</title><link href="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html" rel="alternate" type="text/html" title="What is mechanistic interpretability?" /><published>2025-05-04T00:00:00+03:00</published><updated>2025-05-04T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/04/What%20is%20Mechanistic%20Interpretability</id><content type="html" xml:base="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html"><![CDATA[<p>If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).</p>

<p>This is very much an oversimplification and gives an idealized view of neural networks. Some of the reasons why this is difficult—borderline impossible—is that a lot of the variables networks learn are entangled. One neuron might correspond to multiple concepts. Or there might be learned features we don’t understand or perhaps there are hidden features that the model uses but we can’t extract. On a philosophical point, that’s a bit similar to why we can’t accurately predict the future. We might think that a specific event is certain because we have controlled all the variables but the future might be dependent on causal variables we don’t know exist (latent variables).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).]]></summary></entry></feed>