<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-06T15:09:55+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ayda Sultan</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">What on mars is a feature?</title><link href="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html" rel="alternate" type="text/html" title="What on mars is a feature?" /><published>2025-05-05T00:00:00+03:00</published><updated>2025-05-05T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/05/What%20on%20Mars%20is%20a%20Feature</id><content type="html" xml:base="http://localhost:4000/2025/05/05/What-on-Mars-is-a-Feature.html"><![CDATA[<p>Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.</p>

<p>The idea of features was created as a quest to define atomic units of analysis for neural network computations. This means that features are not decomposable to other meaningful units. In the activation space, features are represented usually as interpretable directions but can also be abstract. Features can be represented by multiple neurons working together. In some cases, individual neurons can also encode features.</p>

<p>So, do features equate to concepts? No, features are not about you, they’re about the model. Whatever the model uses that causally affects performance is considered a feature. Sometimes it does encode some human-interpretable concept, but many times it does not.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Wouldn’t it have been beautiful if each neuron encoded one and only one concept? If we want to remove a specific learned concept from a model, we could just zero out the weight of the corresponding neuron. Sure, but sadly, there seems to be more concepts than neurons within models. So, instead of relying on individual neurons, models encode many concepts using different combinations of neurons. One neuron can be a part of multiple groups of neurons, so it can activate to multiple concepts.]]></summary></entry><entry><title type="html">What is mechanistic interpretability?</title><link href="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html" rel="alternate" type="text/html" title="What is mechanistic interpretability?" /><published>2025-05-04T00:00:00+03:00</published><updated>2025-05-04T00:00:00+03:00</updated><id>http://localhost:4000/2025/05/04/What%20is%20Mechanistic%20Interpretability</id><content type="html" xml:base="http://localhost:4000/2025/05/04/What-is-Mechanistic-Interpretability.html"><![CDATA[<p>If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).</p>

<p>This is very much an oversimplification and gives an idealized view of neural networks. Some of the reasons why this is difficult—borderline impossible—is that a lot of the variables networks learn are entangled. One neuron might correspond to multiple concepts. Or there might be learned features we don’t understand or perhaps there are hidden features that the model uses but we can’t extract. On a philosophical point, that’s a bit similar to why we can’t accurately predict the future. We might think that a specific event is certain because we have controlled all the variables but the future might be dependent on causal variables we don’t know exist (latent variables).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[If not for deep learning, using computers to perform tasks would require carefully crafting each feature and the interconnection between features. Gradient descent relieves us from this trouble by extracting variables and trying out different combinations of variables to maximize an objective function that we specify. Now the problem is that these variables could be non-causal. Just because a cow always appear in grassy areas in the dataset we feed into our model doesn’t mean that it ceases to be a cow in the desert. Grass, in this case, is a confounding variable rather than a causal one. Mechanistic interpretability aims to understand these variables as well as the algorithms a model uses to perform tasks. These are the same algorithms we would have crafted by hand in the pre-deep learning era. To put it simply, in a hypothetical world where we can completely represent cows using their ‘moo’ sound and their tail, mechanistic interpretability aims to extract a Ax + By + Cz algorithm from the model where x is the ‘moo’ sound, y is the tail and z represents grass. Now, even if the model uses confounding variables like z in this case, mech interp helps identify the confounding variable and allows us to directly edit the model by zeroing out the confounding variable’s weight (C in this case).]]></summary></entry></feed>